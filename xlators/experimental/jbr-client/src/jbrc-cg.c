/*
  Copyright (c) 2013 Red Hat, Inc. <http://www.redhat.com>

  This file is licensed to you under your choice of the GNU Lesser
  General Public License, version 3 or any later version (LGPLv3 or
  later), or the GNU General Public License, version 2 (GPLv2), in all
  cases as published by the Free Software Foundation.
*/

#ifndef _CONFIG_H
#define _CONFIG_H
#include "config.h"
#endif

#include "call-stub.h"
#include "defaults.h"
#include "timer.h"
#include "xlator.h"
#include "jbr-messages.h"
#include "jbrc.h"
#include "statedump.h"

#define SCAR_LIMIT      20
#define HILITE(x)       ("[1;33m"x"[0m")

/*
 * The fops are actually generated by gen-fops.py; the rest was mostly copied
 * from defaults.c (commit cd253754 on 27 August 2013).
 */

enum gf_dht_mem_types_ {
        gf_mt_jbrc_private_t = gf_common_mt_end + 1,
        gf_mt_jbrc_end
};

char            *JBRC_XATTR     = "user.jbr.active";

static inline
xlator_t *
ACTIVE_CHILD (xlator_t *parent)
{
        jbrc_private_t  *priv   = parent->private;

        return priv ? priv->active : FIRST_CHILD(parent);
}

xlator_t *
next_xlator (xlator_t *this, xlator_t *prev)
{
        xlator_list_t   *trav;

        for (trav = this->children; trav; trav = trav->next) {
                if (trav->xlator == prev) {
                        return trav->next ? trav->next->xlator
                                          : this->children->xlator;
                }
        }

        return NULL;
}

void
jbrc_retry_cb (void *cb_arg)
{
        jbrc_local_t    *local  = cb_arg;

        gf_msg (__func__, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                HILITE("retrying %p"), local);
        call_resume_wind(local->stub);
}

/* BEGIN GENERATED CODE - DO NOT MODIFY */
int32_t
jbrc_rename_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * buf,
	struct iatt * preoldparent,
	struct iatt * postoldparent,
	struct iatt * prenewparent,
	struct iatt * postnewparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (rename, frame, op_ret, op_errno,
                                     buf, preoldparent, postoldparent, prenewparent, postnewparent, xdata);
        return 0;
}


int32_t
jbrc_rename_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * oldloc,
	loc_t * newloc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_rename_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->rename,
                           oldloc, newloc, xdata);
        return 0;
}

int32_t
jbrc_rename (call_frame_t *frame, xlator_t *this,
             loc_t * oldloc,
	loc_t * newloc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_rename_stub (frame, jbrc_rename_continue,
                                       oldloc, newloc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_rename_cbk, target_xl,
                    target_xl, target_xl->fops->rename,
                    oldloc, newloc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (rename, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_ipc_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (ipc, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_ipc_continue (call_frame_t *frame, xlator_t *this,
                      int32_t op,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_ipc_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->ipc,
                           op, xdata);
        return 0;
}

int32_t
jbrc_ipc (call_frame_t *frame, xlator_t *this,
             int32_t op,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_ipc_stub (frame, jbrc_ipc_continue,
                                       op, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_ipc_cbk, target_xl,
                    target_xl, target_xl->fops->ipc,
                    op, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (ipc, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_setactivelk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (setactivelk, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_setactivelk_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	lock_migration_info_t * locklist,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_setactivelk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->setactivelk,
                           loc, locklist, xdata);
        return 0;
}

int32_t
jbrc_setactivelk (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	lock_migration_info_t * locklist,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_setactivelk_stub (frame, jbrc_setactivelk_continue,
                                       loc, locklist, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_setactivelk_cbk, target_xl,
                    target_xl, target_xl->fops->setactivelk,
                    loc, locklist, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (setactivelk, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_flush_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (flush, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_flush_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_flush_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->flush,
                           fd, xdata);
        return 0;
}

int32_t
jbrc_flush (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_flush_stub (frame, jbrc_flush_continue,
                                       fd, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_flush_cbk, target_xl,
                    target_xl, target_xl->fops->flush,
                    fd, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (flush, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_readdir_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 gf_dirent_t * entries,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (readdir, frame, op_ret, op_errno,
                                     entries, xdata);
        return 0;
}


int32_t
jbrc_readdir_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	size_t size,
	off_t off,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_readdir_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->readdir,
                           fd, size, off, xdata);
        return 0;
}

int32_t
jbrc_readdir (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	size_t size,
	off_t off,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_readdir_stub (frame, jbrc_readdir_continue,
                                       fd, size, off, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_readdir_cbk, target_xl,
                    target_xl, target_xl->fops->readdir,
                    fd, size, off, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (readdir, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_setxattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (setxattr, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_setxattr_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	dict_t * dict,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_setxattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->setxattr,
                           loc, dict, flags, xdata);
        return 0;
}

int32_t
jbrc_setxattr (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	dict_t * dict,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_setxattr_stub (frame, jbrc_setxattr_continue,
                                       loc, dict, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_setxattr_cbk, target_xl,
                    target_xl, target_xl->fops->setxattr,
                    loc, dict, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (setxattr, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_mknod_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (mknod, frame, op_ret, op_errno,
                                     inode, buf, preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_mknod_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	mode_t mode,
	dev_t rdev,
	mode_t umask,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_mknod_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->mknod,
                           loc, mode, rdev, umask, xdata);
        return 0;
}

int32_t
jbrc_mknod (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	mode_t mode,
	dev_t rdev,
	mode_t umask,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_mknod_stub (frame, jbrc_mknod_continue,
                                       loc, mode, rdev, umask, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_mknod_cbk, target_xl,
                    target_xl, target_xl->fops->mknod,
                    loc, mode, rdev, umask, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (mknod, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fsetxattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fsetxattr, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_fsetxattr_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	dict_t * dict,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fsetxattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fsetxattr,
                           fd, dict, flags, xdata);
        return 0;
}

int32_t
jbrc_fsetxattr (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	dict_t * dict,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fsetxattr_stub (frame, jbrc_fsetxattr_continue,
                                       fd, dict, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fsetxattr_cbk, target_xl,
                    target_xl, target_xl->fops->fsetxattr,
                    fd, dict, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fsetxattr, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_readv_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iovec * vector,
	int32_t count,
	struct iatt * stbuf,
	struct iobref * iobref,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (readv, frame, op_ret, op_errno,
                                     vector, count, stbuf, iobref, xdata);
        return 0;
}


int32_t
jbrc_readv_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	size_t size,
	off_t offset,
	uint32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_readv_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->readv,
                           fd, size, offset, flags, xdata);
        return 0;
}

int32_t
jbrc_readv (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	size_t size,
	off_t offset,
	uint32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_readv_stub (frame, jbrc_readv_continue,
                                       fd, size, offset, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_readv_cbk, target_xl,
                    target_xl, target_xl->fops->readv,
                    fd, size, offset, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (readv, frame, -1, ENOMEM,
                             NULL, -1, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_inodelk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (inodelk, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_inodelk_continue (call_frame_t *frame, xlator_t *this,
                      const char * volume,
	loc_t * loc,
	int32_t cmd,
	struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_inodelk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->inodelk,
                           volume, loc, cmd, lock, xdata);
        return 0;
}

int32_t
jbrc_inodelk (call_frame_t *frame, xlator_t *this,
             const char * volume,
	loc_t * loc,
	int32_t cmd,
	struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_inodelk_stub (frame, jbrc_inodelk_continue,
                                       volume, loc, cmd, lock, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_inodelk_cbk, target_xl,
                    target_xl, target_xl->fops->inodelk,
                    volume, loc, cmd, lock, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (inodelk, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_fremovexattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fremovexattr, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_fremovexattr_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fremovexattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fremovexattr,
                           fd, name, xdata);
        return 0;
}

int32_t
jbrc_fremovexattr (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fremovexattr_stub (frame, jbrc_fremovexattr_continue,
                                       fd, name, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fremovexattr_cbk, target_xl,
                    target_xl, target_xl->fops->fremovexattr,
                    fd, name, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fremovexattr, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_open_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (open, frame, op_ret, op_errno,
                                     fd, xdata);
        return 0;
}


int32_t
jbrc_open_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	int32_t flags,
	fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_open_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->open,
                           loc, flags, fd, xdata);
        return 0;
}

int32_t
jbrc_open (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	int32_t flags,
	fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_open_stub (frame, jbrc_open_continue,
                                       loc, flags, fd, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_open_cbk, target_xl,
                    target_xl, target_xl->fops->open,
                    loc, flags, fd, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (open, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_xattrop_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (xattrop, frame, op_ret, op_errno,
                                     dict, xdata);
        return 0;
}


int32_t
jbrc_xattrop_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	gf_xattrop_flags_t flags,
	dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_xattrop_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->xattrop,
                           loc, flags, dict, xdata);
        return 0;
}

int32_t
jbrc_xattrop (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	gf_xattrop_flags_t flags,
	dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_xattrop_stub (frame, jbrc_xattrop_continue,
                                       loc, flags, dict, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_xattrop_cbk, target_xl,
                    target_xl, target_xl->fops->xattrop,
                    loc, flags, dict, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (xattrop, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_entrylk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (entrylk, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_entrylk_continue (call_frame_t *frame, xlator_t *this,
                      const char * volume,
	loc_t * loc,
	const char * basename,
	entrylk_cmd cmd,
	entrylk_type type,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_entrylk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->entrylk,
                           volume, loc, basename, cmd, type, xdata);
        return 0;
}

int32_t
jbrc_entrylk (call_frame_t *frame, xlator_t *this,
             const char * volume,
	loc_t * loc,
	const char * basename,
	entrylk_cmd cmd,
	entrylk_type type,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_entrylk_stub (frame, jbrc_entrylk_continue,
                                       volume, loc, basename, cmd, type, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_entrylk_cbk, target_xl,
                    target_xl, target_xl->fops->entrylk,
                    volume, loc, basename, cmd, type, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (entrylk, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_getactivelk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 lock_migration_info_t * locklist,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (getactivelk, frame, op_ret, op_errno,
                                     locklist, xdata);
        return 0;
}


int32_t
jbrc_getactivelk_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_getactivelk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->getactivelk,
                           loc, xdata);
        return 0;
}

int32_t
jbrc_getactivelk (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_getactivelk_stub (frame, jbrc_getactivelk_continue,
                                       loc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_getactivelk_cbk, target_xl,
                    target_xl, target_xl->fops->getactivelk,
                    loc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (getactivelk, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_finodelk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (finodelk, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_finodelk_continue (call_frame_t *frame, xlator_t *this,
                      const char * volume,
	fd_t * fd,
	int32_t cmd,
	struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_finodelk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->finodelk,
                           volume, fd, cmd, lock, xdata);
        return 0;
}

int32_t
jbrc_finodelk (call_frame_t *frame, xlator_t *this,
             const char * volume,
	fd_t * fd,
	int32_t cmd,
	struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_finodelk_stub (frame, jbrc_finodelk_continue,
                                       volume, fd, cmd, lock, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_finodelk_cbk, target_xl,
                    target_xl, target_xl->fops->finodelk,
                    volume, fd, cmd, lock, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (finodelk, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_create_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 fd_t * fd,
	inode_t * inode,
	struct iatt * buf,
	struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (create, frame, op_ret, op_errno,
                                     fd, inode, buf, preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_create_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	int32_t flags,
	mode_t mode,
	mode_t umask,
	fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_create_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->create,
                           loc, flags, mode, umask, fd, xdata);
        return 0;
}

int32_t
jbrc_create (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	int32_t flags,
	mode_t mode,
	mode_t umask,
	fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_create_stub (frame, jbrc_create_continue,
                                       loc, flags, mode, umask, fd, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_create_cbk, target_xl,
                    target_xl, target_xl->fops->create,
                    loc, flags, mode, umask, fd, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (create, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_discard_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * pre,
	struct iatt * post,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (discard, frame, op_ret, op_errno,
                                     pre, post, xdata);
        return 0;
}


int32_t
jbrc_discard_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	off_t offset,
	size_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_discard_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->discard,
                           fd, offset, len, xdata);
        return 0;
}

int32_t
jbrc_discard (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	off_t offset,
	size_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_discard_stub (frame, jbrc_discard_continue,
                                       fd, offset, len, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_discard_cbk, target_xl,
                    target_xl, target_xl->fops->discard,
                    fd, offset, len, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (discard, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_mkdir_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (mkdir, frame, op_ret, op_errno,
                                     inode, buf, preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_mkdir_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	mode_t mode,
	mode_t umask,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_mkdir_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->mkdir,
                           loc, mode, umask, xdata);
        return 0;
}

int32_t
jbrc_mkdir (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	mode_t mode,
	mode_t umask,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_mkdir_stub (frame, jbrc_mkdir_continue,
                                       loc, mode, umask, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_mkdir_cbk, target_xl,
                    target_xl, target_xl->fops->mkdir,
                    loc, mode, umask, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (mkdir, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_lk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (lk, frame, op_ret, op_errno,
                                     lock, xdata);
        return 0;
}


int32_t
jbrc_lk_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	int32_t cmd,
	struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_lk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->lk,
                           fd, cmd, lock, xdata);
        return 0;
}

int32_t
jbrc_lk (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	int32_t cmd,
	struct gf_flock * lock,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_lk_stub (frame, jbrc_lk_continue,
                                       fd, cmd, lock, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_lk_cbk, target_xl,
                    target_xl, target_xl->fops->lk,
                    fd, cmd, lock, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (lk, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_writev_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * prebuf,
	struct iatt * postbuf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (writev, frame, op_ret, op_errno,
                                     prebuf, postbuf, xdata);
        return 0;
}


int32_t
jbrc_writev_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	struct iovec * vector,
	int32_t count,
	off_t off,
	uint32_t flags,
	struct iobref * iobref,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_writev_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->writev,
                           fd, vector, count, off, flags, iobref, xdata);
        return 0;
}

int32_t
jbrc_writev (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	struct iovec * vector,
	int32_t count,
	off_t off,
	uint32_t flags,
	struct iobref * iobref,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_writev_stub (frame, jbrc_writev_continue,
                                       fd, vector, count, off, flags, iobref, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_writev_cbk, target_xl,
                    target_xl, target_xl->fops->writev,
                    fd, vector, count, off, flags, iobref, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (writev, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_access_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (access, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_access_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	int32_t mask,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_access_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->access,
                           loc, mask, xdata);
        return 0;
}

int32_t
jbrc_access (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	int32_t mask,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_access_stub (frame, jbrc_access_continue,
                                       loc, mask, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_access_cbk, target_xl,
                    target_xl, target_xl->fops->access,
                    loc, mask, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (access, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_lookup_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	dict_t * xdata,
	struct iatt * postparent)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (lookup, frame, op_ret, op_errno,
                                     inode, buf, xdata, postparent);
        return 0;
}


int32_t
jbrc_lookup_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_lookup_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->lookup,
                           loc, xdata);
        return 0;
}

int32_t
jbrc_lookup (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_lookup_stub (frame, jbrc_lookup_continue,
                                       loc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_lookup_cbk, target_xl,
                    target_xl, target_xl->fops->lookup,
                    loc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (lookup, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_rmdir_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (rmdir, frame, op_ret, op_errno,
                                     preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_rmdir_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_rmdir_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->rmdir,
                           loc, flags, xdata);
        return 0;
}

int32_t
jbrc_rmdir (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_rmdir_stub (frame, jbrc_rmdir_continue,
                                       loc, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_rmdir_cbk, target_xl,
                    target_xl, target_xl->fops->rmdir,
                    loc, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (rmdir, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fallocate_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * pre,
	struct iatt * post,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fallocate, frame, op_ret, op_errno,
                                     pre, post, xdata);
        return 0;
}


int32_t
jbrc_fallocate_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	int32_t keep_size,
	off_t offset,
	size_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fallocate_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fallocate,
                           fd, keep_size, offset, len, xdata);
        return 0;
}

int32_t
jbrc_fallocate (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	int32_t keep_size,
	off_t offset,
	size_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fallocate_stub (frame, jbrc_fallocate_continue,
                                       fd, keep_size, offset, len, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fallocate_cbk, target_xl,
                    target_xl, target_xl->fops->fallocate,
                    fd, keep_size, offset, len, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fallocate, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fstat_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * buf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fstat, frame, op_ret, op_errno,
                                     buf, xdata);
        return 0;
}


int32_t
jbrc_fstat_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fstat_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fstat,
                           fd, xdata);
        return 0;
}

int32_t
jbrc_fstat (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fstat_stub (frame, jbrc_fstat_continue,
                                       fd, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fstat_cbk, target_xl,
                    target_xl, target_xl->fops->fstat,
                    fd, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fstat, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_lease_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct gf_lease * lease,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (lease, frame, op_ret, op_errno,
                                     lease, xdata);
        return 0;
}


int32_t
jbrc_lease_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	struct gf_lease * lease,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_lease_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->lease,
                           loc, lease, xdata);
        return 0;
}

int32_t
jbrc_lease (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	struct gf_lease * lease,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_lease_stub (frame, jbrc_lease_continue,
                                       loc, lease, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_lease_cbk, target_xl,
                    target_xl, target_xl->fops->lease,
                    loc, lease, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (lease, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_stat_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * buf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (stat, frame, op_ret, op_errno,
                                     buf, xdata);
        return 0;
}


int32_t
jbrc_stat_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_stat_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->stat,
                           loc, xdata);
        return 0;
}

int32_t
jbrc_stat (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_stat_stub (frame, jbrc_stat_continue,
                                       loc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_stat_cbk, target_xl,
                    target_xl, target_xl->fops->stat,
                    loc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (stat, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_namelink_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * prebuf,
	struct iatt * postbuf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (namelink, frame, op_ret, op_errno,
                                     prebuf, postbuf, xdata);
        return 0;
}


int32_t
jbrc_namelink_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_namelink_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->namelink,
                           loc, xdata);
        return 0;
}

int32_t
jbrc_namelink (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_namelink_stub (frame, jbrc_namelink_continue,
                                       loc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_namelink_cbk, target_xl,
                    target_xl, target_xl->fops->namelink,
                    loc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (namelink, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_truncate_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * prebuf,
	struct iatt * postbuf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (truncate, frame, op_ret, op_errno,
                                     prebuf, postbuf, xdata);
        return 0;
}


int32_t
jbrc_truncate_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	off_t offset,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_truncate_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->truncate,
                           loc, offset, xdata);
        return 0;
}

int32_t
jbrc_truncate (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	off_t offset,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_truncate_stub (frame, jbrc_truncate_continue,
                                       loc, offset, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_truncate_cbk, target_xl,
                    target_xl, target_xl->fops->truncate,
                    loc, offset, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (truncate, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_getxattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (getxattr, frame, op_ret, op_errno,
                                     dict, xdata);
        return 0;
}


int32_t
jbrc_getxattr_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_getxattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->getxattr,
                           loc, name, xdata);
        return 0;
}

int32_t
jbrc_getxattr (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_getxattr_stub (frame, jbrc_getxattr_continue,
                                       loc, name, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_getxattr_cbk, target_xl,
                    target_xl, target_xl->fops->getxattr,
                    loc, name, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (getxattr, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_symlink_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (symlink, frame, op_ret, op_errno,
                                     inode, buf, preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_symlink_continue (call_frame_t *frame, xlator_t *this,
                      const char * linkpath,
	loc_t * loc,
	mode_t umask,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_symlink_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->symlink,
                           linkpath, loc, umask, xdata);
        return 0;
}

int32_t
jbrc_symlink (call_frame_t *frame, xlator_t *this,
             const char * linkpath,
	loc_t * loc,
	mode_t umask,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_symlink_stub (frame, jbrc_symlink_continue,
                                       linkpath, loc, umask, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_symlink_cbk, target_xl,
                    target_xl, target_xl->fops->symlink,
                    linkpath, loc, umask, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (symlink, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_zerofill_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * pre,
	struct iatt * post,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (zerofill, frame, op_ret, op_errno,
                                     pre, post, xdata);
        return 0;
}


int32_t
jbrc_zerofill_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	off_t offset,
	off_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_zerofill_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->zerofill,
                           fd, offset, len, xdata);
        return 0;
}

int32_t
jbrc_zerofill (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	off_t offset,
	off_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_zerofill_stub (frame, jbrc_zerofill_continue,
                                       fd, offset, len, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_zerofill_cbk, target_xl,
                    target_xl, target_xl->fops->zerofill,
                    fd, offset, len, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (zerofill, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fsyncdir_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fsyncdir, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_fsyncdir_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fsyncdir_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fsyncdir,
                           fd, flags, xdata);
        return 0;
}

int32_t
jbrc_fsyncdir (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fsyncdir_stub (frame, jbrc_fsyncdir_continue,
                                       fd, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fsyncdir_cbk, target_xl,
                    target_xl, target_xl->fops->fsyncdir,
                    fd, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fsyncdir, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_fgetxattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fgetxattr, frame, op_ret, op_errno,
                                     dict, xdata);
        return 0;
}


int32_t
jbrc_fgetxattr_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fgetxattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fgetxattr,
                           fd, name, xdata);
        return 0;
}

int32_t
jbrc_fgetxattr (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fgetxattr_stub (frame, jbrc_fgetxattr_continue,
                                       fd, name, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fgetxattr_cbk, target_xl,
                    target_xl, target_xl->fops->fgetxattr,
                    fd, name, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fgetxattr, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_readdirp_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 gf_dirent_t * entries,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (readdirp, frame, op_ret, op_errno,
                                     entries, xdata);
        return 0;
}


int32_t
jbrc_readdirp_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	size_t size,
	off_t off,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_readdirp_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->readdirp,
                           fd, size, off, xdata);
        return 0;
}

int32_t
jbrc_readdirp (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	size_t size,
	off_t off,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_readdirp_stub (frame, jbrc_readdirp_continue,
                                       fd, size, off, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_readdirp_cbk, target_xl,
                    target_xl, target_xl->fops->readdirp,
                    fd, size, off, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (readdirp, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_link_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (link, frame, op_ret, op_errno,
                                     inode, buf, preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_link_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * oldloc,
	loc_t * newloc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_link_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->link,
                           oldloc, newloc, xdata);
        return 0;
}

int32_t
jbrc_link (call_frame_t *frame, xlator_t *this,
             loc_t * oldloc,
	loc_t * newloc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_link_stub (frame, jbrc_link_continue,
                                       oldloc, newloc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_link_cbk, target_xl,
                    target_xl, target_xl->fops->link,
                    oldloc, newloc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (link, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fxattrop_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fxattrop, frame, op_ret, op_errno,
                                     dict, xdata);
        return 0;
}


int32_t
jbrc_fxattrop_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	gf_xattrop_flags_t flags,
	dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fxattrop_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fxattrop,
                           fd, flags, dict, xdata);
        return 0;
}

int32_t
jbrc_fxattrop (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	gf_xattrop_flags_t flags,
	dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fxattrop_stub (frame, jbrc_fxattrop_continue,
                                       fd, flags, dict, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fxattrop_cbk, target_xl,
                    target_xl, target_xl->fops->fxattrop,
                    fd, flags, dict, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fxattrop, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_ftruncate_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * prebuf,
	struct iatt * postbuf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (ftruncate, frame, op_ret, op_errno,
                                     prebuf, postbuf, xdata);
        return 0;
}


int32_t
jbrc_ftruncate_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	off_t offset,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_ftruncate_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->ftruncate,
                           fd, offset, xdata);
        return 0;
}

int32_t
jbrc_ftruncate (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	off_t offset,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_ftruncate_stub (frame, jbrc_ftruncate_continue,
                                       fd, offset, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_ftruncate_cbk, target_xl,
                    target_xl, target_xl->fops->ftruncate,
                    fd, offset, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (ftruncate, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_put_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (put, frame, op_ret, op_errno,
                                     inode, buf, preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_put_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	mode_t mode,
	mode_t umask,
	uint32_t flags,
	struct iovec * vector,
	int32_t count,
	off_t off,
	struct iobref * iobref,
	dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_put_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->put,
                           loc, mode, umask, flags, vector, count, off, iobref, dict, xdata);
        return 0;
}

int32_t
jbrc_put (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	mode_t mode,
	mode_t umask,
	uint32_t flags,
	struct iovec * vector,
	int32_t count,
	off_t off,
	struct iobref * iobref,
	dict_t * dict,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_put_stub (frame, jbrc_put_continue,
                                       loc, mode, umask, flags, vector, count, off, iobref, dict, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_put_cbk, target_xl,
                    target_xl, target_xl->fops->put,
                    loc, mode, umask, flags, vector, count, off, iobref, dict, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (put, frame, -1, ENOMEM,
                             NULL, NULL, NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_rchecksum_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 uint32_t weak_cksum,
	uint8_t * strong_cksum,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (rchecksum, frame, op_ret, op_errno,
                                     weak_cksum, strong_cksum, xdata);
        return 0;
}


int32_t
jbrc_rchecksum_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	off_t offset,
	int32_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_rchecksum_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->rchecksum,
                           fd, offset, len, xdata);
        return 0;
}

int32_t
jbrc_rchecksum (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	off_t offset,
	int32_t len,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_rchecksum_stub (frame, jbrc_rchecksum_continue,
                                       fd, offset, len, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_rchecksum_cbk, target_xl,
                    target_xl, target_xl->fops->rchecksum,
                    fd, offset, len, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (rchecksum, frame, -1, ENOMEM,
                             -1, NULL, NULL);
        return 0;
}


int32_t
jbrc_unlink_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * preparent,
	struct iatt * postparent,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (unlink, frame, op_ret, op_errno,
                                     preparent, postparent, xdata);
        return 0;
}


int32_t
jbrc_unlink_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_unlink_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->unlink,
                           loc, flags, xdata);
        return 0;
}

int32_t
jbrc_unlink (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_unlink_stub (frame, jbrc_unlink_continue,
                                       loc, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_unlink_cbk, target_xl,
                    target_xl, target_xl->fops->unlink,
                    loc, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (unlink, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fentrylk_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fentrylk, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_fentrylk_continue (call_frame_t *frame, xlator_t *this,
                      const char * volume,
	fd_t * fd,
	const char * basename,
	entrylk_cmd cmd,
	entrylk_type type,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fentrylk_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fentrylk,
                           volume, fd, basename, cmd, type, xdata);
        return 0;
}

int32_t
jbrc_fentrylk (call_frame_t *frame, xlator_t *this,
             const char * volume,
	fd_t * fd,
	const char * basename,
	entrylk_cmd cmd,
	entrylk_type type,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fentrylk_stub (frame, jbrc_fentrylk_continue,
                                       volume, fd, basename, cmd, type, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fentrylk_cbk, target_xl,
                    target_xl, target_xl->fops->fentrylk,
                    volume, fd, basename, cmd, type, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fentrylk, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_setattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * statpre,
	struct iatt * statpost,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (setattr, frame, op_ret, op_errno,
                                     statpre, statpost, xdata);
        return 0;
}


int32_t
jbrc_setattr_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	struct iatt * stbuf,
	int32_t valid,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_setattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->setattr,
                           loc, stbuf, valid, xdata);
        return 0;
}

int32_t
jbrc_setattr (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	struct iatt * stbuf,
	int32_t valid,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_setattr_stub (frame, jbrc_setattr_continue,
                                       loc, stbuf, valid, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_setattr_cbk, target_xl,
                    target_xl, target_xl->fops->setattr,
                    loc, stbuf, valid, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (setattr, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_fsync_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * prebuf,
	struct iatt * postbuf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fsync, frame, op_ret, op_errno,
                                     prebuf, postbuf, xdata);
        return 0;
}


int32_t
jbrc_fsync_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fsync_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fsync,
                           fd, flags, xdata);
        return 0;
}

int32_t
jbrc_fsync (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	int32_t flags,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fsync_stub (frame, jbrc_fsync_continue,
                                       fd, flags, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fsync_cbk, target_xl,
                    target_xl, target_xl->fops->fsync,
                    fd, flags, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fsync, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_statfs_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct statvfs * buf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (statfs, frame, op_ret, op_errno,
                                     buf, xdata);
        return 0;
}


int32_t
jbrc_statfs_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_statfs_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->statfs,
                           loc, xdata);
        return 0;
}

int32_t
jbrc_statfs (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_statfs_stub (frame, jbrc_statfs_continue,
                                       loc, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_statfs_cbk, target_xl,
                    target_xl, target_xl->fops->statfs,
                    loc, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (statfs, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_seek_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 off_t offset,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (seek, frame, op_ret, op_errno,
                                     offset, xdata);
        return 0;
}


int32_t
jbrc_seek_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	off_t offset,
	gf_seek_what_t what,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_seek_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->seek,
                           fd, offset, what, xdata);
        return 0;
}

int32_t
jbrc_seek (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	off_t offset,
	gf_seek_what_t what,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_seek_stub (frame, jbrc_seek_continue,
                                       fd, offset, what, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_seek_cbk, target_xl,
                    target_xl, target_xl->fops->seek,
                    fd, offset, what, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (seek, frame, -1, ENOMEM,
                             -1, NULL);
        return 0;
}


int32_t
jbrc_fsetattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 struct iatt * statpre,
	struct iatt * statpost,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (fsetattr, frame, op_ret, op_errno,
                                     statpre, statpost, xdata);
        return 0;
}


int32_t
jbrc_fsetattr_continue (call_frame_t *frame, xlator_t *this,
                      fd_t * fd,
	struct iatt * stbuf,
	int32_t valid,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_fsetattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->fsetattr,
                           fd, stbuf, valid, xdata);
        return 0;
}

int32_t
jbrc_fsetattr (call_frame_t *frame, xlator_t *this,
             fd_t * fd,
	struct iatt * stbuf,
	int32_t valid,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_fsetattr_stub (frame, jbrc_fsetattr_continue,
                                       fd, stbuf, valid, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_fsetattr_cbk, target_xl,
                    target_xl, target_xl->fops->fsetattr,
                    fd, stbuf, valid, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (fsetattr, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_opendir_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (opendir, frame, op_ret, op_errno,
                                     fd, xdata);
        return 0;
}


int32_t
jbrc_opendir_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_opendir_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->opendir,
                           loc, fd, xdata);
        return 0;
}

int32_t
jbrc_opendir (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	fd_t * fd,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_opendir_stub (frame, jbrc_opendir_continue,
                                       loc, fd, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_opendir_cbk, target_xl,
                    target_xl, target_xl->fops->opendir,
                    loc, fd, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (opendir, frame, -1, ENOMEM,
                             NULL, NULL);
        return 0;
}


int32_t
jbrc_readlink_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 const char * path,
	struct iatt * buf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (readlink, frame, op_ret, op_errno,
                                     path, buf, xdata);
        return 0;
}


int32_t
jbrc_readlink_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	size_t size,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_readlink_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->readlink,
                           loc, size, xdata);
        return 0;
}

int32_t
jbrc_readlink (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	size_t size,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_readlink_stub (frame, jbrc_readlink_continue,
                                       loc, size, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_readlink_cbk, target_xl,
                    target_xl, target_xl->fops->readlink,
                    loc, size, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (readlink, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


int32_t
jbrc_removexattr_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (removexattr, frame, op_ret, op_errno,
                                     xdata);
        return 0;
}


int32_t
jbrc_removexattr_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_removexattr_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->removexattr,
                           loc, name, xdata);
        return 0;
}

int32_t
jbrc_removexattr (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	const char * name,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_removexattr_stub (frame, jbrc_removexattr_continue,
                                       loc, name, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_removexattr_cbk, target_xl,
                    target_xl, target_xl->fops->removexattr,
                    loc, name, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (removexattr, frame, -1, ENOMEM,
                             NULL);
        return 0;
}


int32_t
jbrc_icreate_cbk (call_frame_t *frame, void *cookie, xlator_t *this,
                 int32_t op_ret, int32_t op_errno,
                 inode_t * inode,
	struct iatt * buf,
	dict_t * xdata)
{
        jbrc_local_t    *local          = frame->local;
        xlator_t        *last_xl        = cookie;
        xlator_t        *next_xl;
        jbrc_private_t  *priv           = this->private;
        struct timespec spec;

        if (op_ret != (-1)) {
                if (local->scars) {
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_RETRY_MSG,
                                HILITE("retried %p OK"), frame->local);
                }
                priv->active = last_xl;
                goto unwind;
        }
        if ((op_errno != EREMOTE) && (op_errno != ENOTCONN)) {
                goto unwind;
        }

        /* TBD: get leader ID from xdata? */
        next_xl = next_xlator(this, last_xl);
        /*
         * We can't just give up after we've tried all bricks, because it's
         * quite likely that a new leader election just hasn't finished yet.
         * We also shouldn't retry endlessly, and especially not at a high
         * rate, but that's good enough while we work on other things.
         *
         * TBD: implement slow/finite retry via a worker thread
         */
        if (!next_xl || (local->scars >= SCAR_LIMIT)) {
                gf_msg (this->name, GF_LOG_DEBUG, 0, J_MSG_RETRY_MSG,
                        HILITE("ran out of retries for %p"), frame->local);
                goto unwind;
        }

        local->curr_xl = next_xl;
        local->scars += 1;
        spec.tv_sec = 1;
        spec.tv_nsec = 0;
        /*
         * WARNING
         *
         * Just calling gf_timer_call_after like this leaves open the
         * possibility that writes will get reordered, if a first write is
         * rescheduled and then a second comes along to find an updated
         * priv->active before the first actually executes.  We might need to
         * implement a stricter (and more complicated) queuing mechanism to
         * ensure absolute consistency in this case.
         */
        if (gf_timer_call_after(this->ctx, spec, jbrc_retry_cb, local)) {
                return 0;
        }

unwind:
        call_stub_destroy(local->stub);
        STACK_UNWIND_STRICT (icreate, frame, op_ret, op_errno,
                                     inode, buf, xdata);
        return 0;
}


int32_t
jbrc_icreate_continue (call_frame_t *frame, xlator_t *this,
                      loc_t * loc,
	mode_t mode,
	dict_t * xdata)
{
        jbrc_local_t    *local  = frame->local;

        STACK_WIND_COOKIE (frame, jbrc_icreate_cbk, local->curr_xl,
                           local->curr_xl, local->curr_xl->fops->icreate,
                           loc, mode, xdata);
        return 0;
}

int32_t
jbrc_icreate (call_frame_t *frame, xlator_t *this,
             loc_t * loc,
	mode_t mode,
	dict_t * xdata)
{
        jbrc_local_t    *local          = NULL;
        xlator_t        *target_xl      = ACTIVE_CHILD(this);

        local = mem_get(this->local_pool);
        if (!local) {
                goto err;
        }

        local->stub = fop_icreate_stub (frame, jbrc_icreate_continue,
                                       loc, mode, xdata);
        if (!local->stub) {
                goto err;
        }
        local->curr_xl = target_xl;
        local->scars = 0;

        frame->local = local;
        STACK_WIND_COOKIE (frame, jbrc_icreate_cbk, target_xl,
                    target_xl, target_xl->fops->icreate,
                    loc, mode, xdata);
        return 0;

err:
        if (local) {
                mem_put(local);
        }
        STACK_UNWIND_STRICT (icreate, frame, -1, ENOMEM,
                             NULL, NULL, NULL);
        return 0;
}


/* END GENERATED CODE */

int32_t
jbrc_forget (xlator_t *this, inode_t *inode)
{
        gf_msg_callingfn (this->name, GF_LOG_WARNING, 0, J_MSG_INIT_FAIL,
                          "xlator does not implement forget_cbk");
        return 0;
}


int32_t
jbrc_releasedir (xlator_t *this, fd_t *fd)
{
        gf_msg_callingfn (this->name, GF_LOG_WARNING, 0, J_MSG_INIT_FAIL,
                          "xlator does not implement releasedir_cbk");
        return 0;
}

int32_t
jbrc_release (xlator_t *this, fd_t *fd)
{
        gf_msg_callingfn (this->name, GF_LOG_WARNING, 0, J_MSG_INIT_FAIL,
                          "xlator does not implement release_cbk");
        return 0;
}

struct xlator_fops fops = {
        .lookup         = jbrc_lookup,
        .stat           = jbrc_stat,
        .fstat          = jbrc_fstat,
        .truncate       = jbrc_truncate,
        .ftruncate      = jbrc_ftruncate,
        .access         = jbrc_access,
        .readlink       = jbrc_readlink,
        .mknod          = jbrc_mknod,
        .mkdir          = jbrc_mkdir,
        .unlink         = jbrc_unlink,
        .rmdir          = jbrc_rmdir,
        .symlink        = jbrc_symlink,
        .rename         = jbrc_rename,
        .link           = jbrc_link,
        .create         = jbrc_create,
        .open           = jbrc_open,
        .readv          = jbrc_readv,
        .writev         = jbrc_writev,
        .flush          = jbrc_flush,
        .fsync          = jbrc_fsync,
        .opendir        = jbrc_opendir,
        .readdir        = jbrc_readdir,
        .readdirp       = jbrc_readdirp,
        .fsyncdir       = jbrc_fsyncdir,
        .statfs         = jbrc_statfs,
        .setxattr       = jbrc_setxattr,
        .getxattr       = jbrc_getxattr,
        .fsetxattr      = jbrc_fsetxattr,
        .fgetxattr      = jbrc_fgetxattr,
        .removexattr    = jbrc_removexattr,
        .fremovexattr   = jbrc_fremovexattr,
        .lk             = jbrc_lk,
        .inodelk        = jbrc_inodelk,
        .finodelk       = jbrc_finodelk,
        .entrylk        = jbrc_entrylk,
        .fentrylk       = jbrc_fentrylk,
        .rchecksum      = jbrc_rchecksum,
        .xattrop        = jbrc_xattrop,
        .fxattrop       = jbrc_fxattrop,
        .setattr        = jbrc_setattr,
        .fsetattr       = jbrc_fsetattr,
	.fallocate	= jbrc_fallocate,
	.discard        = jbrc_discard,
};

struct xlator_cbks cbks = {
};


int32_t
mem_acct_init (xlator_t *this)
{
        int     ret = -1;

        GF_VALIDATE_OR_GOTO ("jbrc", this, out);

        ret = xlator_mem_acct_init (this, gf_mt_jbrc_end + 1);

        if (ret != 0) {
                gf_msg (this->name, GF_LOG_ERROR, ENOMEM, J_MSG_MEM_ERR,
                        "Memory accounting init failed");
                return ret;
        }
out:
        return ret;
}


int32_t
jbrc_init (xlator_t *this)
{
        jbrc_private_t  *priv   = NULL;
        xlator_list_t   *trav   = NULL;

        this->local_pool = mem_pool_new (jbrc_local_t, 128);
        if (!this->local_pool) {
                gf_msg (this->name, GF_LOG_ERROR, ENOMEM, J_MSG_MEM_ERR,
                        "failed to create jbrc_local_t pool");
                goto err;
        }

        priv = GF_CALLOC (1, sizeof (*priv), gf_mt_jbrc_private_t);
        if (!priv) {
                goto err;
        }

        for (trav = this->children; trav; trav = trav->next) {
                ++(priv->n_children);
        }

        priv->active = FIRST_CHILD(this);
        this->private = priv;
        return 0;

err:
        if (priv) {
                GF_FREE(priv);
        }
        return -1;
}

void
jbrc_fini (xlator_t *this)
{
        GF_FREE(this->private);
}

int
jbrc_get_child_index (xlator_t *this, xlator_t *kid)
{
        xlator_list_t   *trav;
        int             retval = -1;

        for (trav = this->children; trav; trav = trav->next) {
                ++retval;
                if (trav->xlator == kid) {
                        return retval;
                }
        }

        return -1;
}

uint8_t
jbrc_count_up_kids (jbrc_private_t *priv)
{
        uint8_t         retval  = 0;
        uint8_t         i;

        for (i = 0; i < priv->n_children; ++i) {
                if (priv->kid_state & (1 << i)) {
                        ++retval;
                }
        }

        return retval;
}

int32_t
jbrc_notify (xlator_t *this, int32_t event, void *data, ...)
{
        int32_t           ret        = 0;
        int32_t           index      = 0;
        jbrc_private_t   *priv       = NULL;

        GF_VALIDATE_OR_GOTO (THIS->name, this, out);
        priv = this->private;
        GF_VALIDATE_OR_GOTO (this->name, priv, out);

        switch (event) {
        case GF_EVENT_CHILD_UP:
                index = jbrc_get_child_index(this, data);
                if (index >= 0) {
                        priv->kid_state |= (1 << index);
                        priv->up_children = jbrc_count_up_kids(priv);
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_GENERIC,
                                "got CHILD_UP for %s, now %u kids",
                                ((xlator_t *)data)->name,
                                priv->up_children);
                }
                ret = default_notify (this, event, data);
                break;
        case GF_EVENT_CHILD_DOWN:
                index = jbrc_get_child_index(this, data);
                if (index >= 0) {
                        priv->kid_state &= ~(1 << index);
                        priv->up_children = jbrc_count_up_kids(priv);
                        gf_msg (this->name, GF_LOG_INFO, 0, J_MSG_GENERIC,
                                "got CHILD_DOWN for %s, now %u kids",
                                ((xlator_t *)data)->name,
                                priv->up_children);
                }
                break;
        default:
                ret = default_notify (this, event, data);
        }

out:
        return ret;
}

int
jbrc_priv_dump (xlator_t *this)
{
        jbrc_private_t     *priv = NULL;
        char                key_prefix[GF_DUMP_MAX_BUF_LEN];
        xlator_list_t      *trav = NULL;
        int32_t             i    = -1;

        GF_VALIDATE_OR_GOTO (THIS->name, this, out);
        priv = this->private;
        GF_VALIDATE_OR_GOTO (this->name, priv, out);

        snprintf(key_prefix, GF_DUMP_MAX_BUF_LEN, "%s.%s",
                 this->type, this->name);
        gf_proc_dump_add_section(key_prefix);

        gf_proc_dump_write("up_children", "%u", priv->up_children);

        for (trav = this->children, i = 0; trav; trav = trav->next, i++) {
                snprintf(key_prefix, GF_DUMP_MAX_BUF_LEN, "child_%d", i);
                gf_proc_dump_write(key_prefix, "%s", trav->xlator->name);
        }

out:
        return 0;
}

struct xlator_dumpops dumpops = {
        .priv       = jbrc_priv_dump,
};

class_methods_t class_methods = {
        .init           = jbrc_init,
        .fini           = jbrc_fini,
        .notify         = jbrc_notify,
};

struct volume_options options[] = {
	{ .key = {NULL} },
};
